{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "\n",
        "# Fetch and extract URLs from sitemaps\n",
        "def fetch_sitemap_urls(sitemaps):\n",
        "    urls = []\n",
        "    for sitemap in sitemaps:\n",
        "        try:\n",
        "            root = ET.fromstring(requests.get(sitemap).content)\n",
        "            urls.extend(url.text for url in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error with sitemap {sitemap}: {e}\")\n",
        "    return urls\n",
        "\n",
        "# Save URLs to CSV\n",
        "def save_urls_to_csv(filename, urls):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        csv.writer(file).writerows([['url']] + [[url] for url in urls])\n",
        "\n",
        "# Input and execution\n",
        "sitemaps = [\"https://acviss.com/page-sitemap.xml\", \"https://blog.acviss.com/sitemap-post.xml\"]\n",
        "all_urls = fetch_sitemap_urls(sitemaps)\n",
        "save_urls_to_csv(\"url.csv\", all_urls)\n",
        "print(f\"Total pages extracted: {len(all_urls)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gas6HB0aIUc",
        "outputId": "ab0e243b-b65b-4507-b9d6-67e5192e6e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages extracted: 328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Fetch blog data\n",
        "def fetch_blog_data(url, word_limit=1000):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.title.get_text(strip=True) if soup.title else \"Title not found\"\n",
        "        content = soup.find('div', class_='main-content') or soup.find('article') or soup.find('section')\n",
        "        text = content.get_text(\" \").strip() if content else \"Content not found\"\n",
        "\n",
        "        return {\"url\": url, \"title\": title, \"content\": \" \".join(text.split()[:word_limit])}\n",
        "    except Exception as e:\n",
        "        return {\"url\": url, \"title\": \"Error\", \"content\": f\"Error: {e}\"}\n",
        "\n",
        "# Scrape URLs from CSV\n",
        "def scrape_urls(file_path, word_limit=1000):\n",
        "    try:\n",
        "        urls = pd.read_csv(file_path)['url']\n",
        "        return pd.DataFrame([fetch_blog_data(url, word_limit) for url in tqdm(urls, desc=\"Scraping URLs\")])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv = '/content/url.csv'\n",
        "    output_csv = '/content/scraped_urls.csv'\n",
        "    word_limit = int(input(\"Enter max words to extract (default 1000): \") or 1000)\n",
        "\n",
        "    print(\"Scraping started...\")\n",
        "    data = scrape_urls(input_csv, word_limit)\n",
        "    if not data.empty:\n",
        "        data.to_csv(output_csv, index=False)\n",
        "        print(f\"Data saved to '{output_csv}'\")\n",
        "    else:\n",
        "        print(\"No data scraped. Check the input file or URLs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YSjBU7daIZy",
        "outputId": "c7266437-89b9-460e-a6a0-4f2278a2ad4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter max words to extract (default 1000): 1000\n",
            "Scraping started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping URLs: 100%|██████████| 328/328 [02:40<00:00,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to '/content/scraped_urls.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake-nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFhLDjiFaIc8",
        "outputId": "304f19a0-b0c3-4d93-e25d-d2bb645ccfd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.6)\n",
            "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from rake_nltk import Rake\n",
        "\n",
        "# Extract keywords from URL\n",
        "def extract_keywords_from_url(url):\n",
        "    try:\n",
        "        path = urlparse(url).path\n",
        "        keywords = re.split(r'[-/]', path)\n",
        "        return [word for word in keywords if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "# Extract keywords using RAKE\n",
        "def extract_keywords_with_rake(text, num_keywords=10):\n",
        "    try:\n",
        "        rake = Rake()\n",
        "        rake.extract_keywords_from_text(text)\n",
        "        return rake.get_ranked_phrases()[:num_keywords]\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "# Generate target keywords\n",
        "def generate_target_keywords(data, num_keywords=10):\n",
        "    data['keywords'] = data.apply(\n",
        "        lambda row: \", \".join(\n",
        "            set(\n",
        "                extract_keywords_from_url(row['url']) +\n",
        "                extract_keywords_with_rake(row['title'], num_keywords) +\n",
        "                extract_keywords_with_rake(row['content'], num_keywords)\n",
        "            )\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    return data\n",
        "\n",
        "# Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/scraped_urls.csv'\n",
        "    output_path = '/content/target_keywords.csv'\n",
        "\n",
        "    try:\n",
        "        print(\"Loading scraped data...\")\n",
        "        scraped_data = pd.read_csv(file_path)\n",
        "\n",
        "        print(\"Generating target keywords...\")\n",
        "        enriched_data = generate_target_keywords(scraped_data)\n",
        "\n",
        "        enriched_data.to_csv(output_path, index=False)\n",
        "        print(f\"Enriched data saved to '{output_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNQtcRY2aIfk",
        "outputId": "ce6cded0-bfb4-4c70-93a1-ba507c0763e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading scraped data...\n",
            "Generating target keywords...\n",
            "Enriched data saved to '/content/target_keywords.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the scraped data\n",
        "file_path = '/content/scraped_urls.csv'  # Update with the actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to extract keywords from text\n",
        "def extract_keywords(text, top_n=10):\n",
        "    if pd.isnull(text):\n",
        "        return []\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_features=top_n)\n",
        "    matrix = vectorizer.fit_transform([text])\n",
        "    keywords = vectorizer.get_feature_names_out()\n",
        "    return list(keywords)\n",
        "\n",
        "# Function to combine and extract target keywords\n",
        "def generate_target_keywords(row, top_n=10):\n",
        "    combined_text = f\"{row['url']} {row['title']} {row['content']}\"\n",
        "    keywords = extract_keywords(combined_text, top_n)\n",
        "    return ', '.join(keywords)\n",
        "\n",
        "# Apply the keyword generation function\n",
        "data['keywords'] = data.apply(lambda row: generate_target_keywords(row, top_n=20), axis=1)\n",
        "\n",
        "# Save the updated data to a new CSV\n",
        "output_file_path = '/content/updated_scraped_urls.csv'  # Update with the desired output path\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Updated data with target keywords saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgyIWqTZaOko",
        "outputId": "1cd28dd2-6356-42b6-ee79-ca2b9c31595c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated data with target keywords saved to /content/updated_scraped_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase, remove stopwords and unwanted terms\n",
        "    stop_words = set(stopwords.words('english')).union({'https', 'com', 'blog', 'www'})\n",
        "    text = re.sub(r'\\W+', ' ', str(text).lower())\n",
        "    return \" \".join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "def calculate_relevance(content, blog_data, title_weight=2, threshold=0.15):\n",
        "    content_cleaned = preprocess_text(content)\n",
        "    blog_data['processed_keywords'] = blog_data['keywords'].apply(preprocess_text)\n",
        "    blog_data['processed_title'] = blog_data['title'].apply(preprocess_text)\n",
        "\n",
        "    combined_data = blog_data['processed_keywords'] + \" \" + blog_data['processed_title'] * title_weight\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(combined_data.tolist() + [content_cleaned])\n",
        "    similarities = cosine_similarity(vectors[-1], vectors[:-1]).flatten()\n",
        "\n",
        "    return similarities\n",
        "\n",
        "def suggest_internal_links(content, blog_data, top_n=30):\n",
        "    relevance_scores = calculate_relevance(content, blog_data)\n",
        "    blog_data['relevance'] = relevance_scores\n",
        "    blog_data = blog_data[blog_data['relevance'] >= 0.15]\n",
        "\n",
        "    # Sort by relevance score\n",
        "    suggestions = blog_data.nlargest(top_n, 'relevance')\n",
        "    suggestions['relevance (%)'] = (suggestions['relevance'] * 100).round(2)\n",
        "\n",
        "    return suggestions[['title', 'url', 'keywords', 'relevance (%)']]\n",
        "\n",
        "# Main workflow\n",
        "def main():\n",
        "    file_path = '/content/updated_scraped_urls.csv'\n",
        "    try:\n",
        "        blog_data = pd.read_csv(file_path)\n",
        "        blog_data.columns = [col.strip().lower() for col in blog_data.columns]\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    new_blog_content = input(\"Enter the new blog content: \").strip()\n",
        "    if not new_blog_content:\n",
        "        print(\"Error: Blog content cannot be empty.\")\n",
        "        return\n",
        "\n",
        "    suggestions = suggest_internal_links(new_blog_content, blog_data)\n",
        "    if suggestions.empty:\n",
        "        print(\"No relevant links found.\")\n",
        "    else:\n",
        "        print(\"\\nSuggested Internal Links:\")\n",
        "        for idx, row in suggestions.iterrows():\n",
        "            print(f\"{idx + 1}. {row['title']} ({row['relevance (%)']}%)\")\n",
        "            print(f\"   URL: {row['url']}\")\n",
        "            print(f\"   Matched Keywords: {row['keywords']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1qoNNaqQaTT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bJzSli3EqX3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}